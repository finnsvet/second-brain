Concurrency is the execution of several tasks simultaneously. It is widely used to improve throughput(by leveraging on multiple processors for a single computation).

Concurrency should not be viewed as a panacea, if a task can be done sequentially, its often simpler and faster to do so.

## 18.2 Tasks and `threads`
A *task* is a concurrent computation. A *thread* is a system level representation of a task. A task can be constructed and launch using a `thread` in `<thread>`:
```c++
void f();

struct F {
	void operator()(); 
};

void user()
{
	thread t1 {f};
	thread t2 {F{}};
	
	t1.join();
	t2.join();
}
```
`join()` ensures we don’t exit user until the specified thread is completed. To joins a thread is to “wait for the thread to terminate”.

Its quite easy to forget to join a thread, hence c++ provides `jthreads`, that implicitly imposes itself to be `join()`ed, by having its destructor handle it (RAII).
```c++
void user()
{
	jthread t1 {f};
	jthread T2 {f{}};
}
```
______________________
Threads of a program share a single address space which differs its from processes, which generally, do no share data. So.. since threads belong to the same address space, they can communicate via shared objects.

Communications in threads is usually controlled by locks and other mechanism to prevent data races. *Data race* occurs when two or more threads try to access mutable data in an uncontrollable fashion.

To explain data races lets consider the implementation of `f()` and `F()`:
```c++
void f() {
	cout << "Hello";
}
void F::operator()() {
	cout << "Parallel World\n";
};
```
As we can see both functions access `std::cout` and since they are ran concurrently with no mechanism to enforce an order on how the mutable object `std::cout` could be accessed, data races might occur; its effects could range from unpredictable nonsensical output, or even the corruption of `cout` because both threads access `cout` simultaneously. A potential outcome of a benign data race:
```bash
PaHerallllel o World!
```
We could have avoided this by using a single thread, or `osyncstream` (enforces controlled concurrent access to `ostream`)

## 18.2.1 Passing Arguments
A task needs arguments to work on, we can easily pass those as arguments:
```c++
void f(vector<double>& v);

struct F {
	vector<double>& v;
	F(vector<double>& vv): v{vv} {}
	void oeprator()()
};

int main()
{
	vector<double> some_vec {1, 2, 3, 4, 5, 6, 7, 8, 9};
	vector<double> vec2 {10, 11, 12, 13, 14};

	jthread t1 {f, ref(some_vec)};   // f(some_vec) execs. in some thread
	jthread t2 {F{vec2}};            // F{vec2}() execs. in some thread
}
```
`ref()`  → `<functional>`

## 18.2.2 Returning Results
```c++
void f(const vector<double>& v, double* res);

class F {
	double* res;
	const vector<double>& vec;
public:
	F(const vector<double>& v, double* r) : vec {v}, res{r} {}
	void operator()();
}

double g(const vector<double>& vec);

void user(vector<double> v1, vector<double>v2, vector<double>v3)
{
	double res1:
	double res2;
	double res3;

	jthread t1 {f, cref(v1), &res1};
	jthread t2 {F{v2, &res2}};
	jthread t3 {[&](){ res3=g(v3); }};

	cout << res1 << res2 << res3; 
}
```

## 18.3 Sharing data
When data needs to be shared between threads, the processes has to be synchronized so that at most one task a  has data. Though there is no problem with multiple threads reading immutable data.

## 18.3.1 `mutexes` and Locks
A `mutex` is a *mutual exclusion object*, Its a key element in sharing of data between threads.  A thread requires a `mutex` using a `lock()` operation. The lock determines when the `mutex` can be released so other threads can access whatever data the `mutex` was safe-guarding.

`scoped_lock<mutex>` for example, acquires  `mutex m`, by invoking `m.lock()`, when this happens other threads trying to access the data binding by `mutex m`, sleeps; awaiting the unlock of the `mutex`. so when The current thread (the one that locked `mutex m` via `scoped_lock`) is done with the data and its thread is about to leave scope, it unlocks  the `mutex` by invoking `m.unlock()`, Thereby waking up other threads that require the data binded to the just-released `mutex` allowing the next thread to access the shared data. These facilities are found in `<mutex>`.

The use of RAII—resource handles, such as `scoped_lock` and `unique_lock`, is simpler and far safer than explicitly locking and unlocking `mutex`es.

We can make the correspondence between mutex and shared objects via language means. Example:  Record with a binded mutex:
```c++
struct Record {
	mutex m;
	// access functions ensure mutex has been acquired before
	// granting access.
private:
	Date data;
}
```
Usage of record with mutually exclusive access via acquiring mutex, enforced:
```cpp

Record rec;

void f(Record& r) {
	scoped_lock lk(r.m);
	Data = r.getData();
	// use Data
}
void g(Record& r) { 
	scoped_lock lk (r.m);
	r.getData() = some_process; 
	// use Data
}

void user()
{
	jthread(f, ref(rec));  // if f acquires the mutex first
	jthread(g, ref(rec));  // g falls asleep waiting to mutex release.
}
```
This enforces that for us to access a Record called, `rec`, we must acquire `rec.rm`. before accessing the rest of `rec`.
____________________
**Deadlocking** can occur when simoultaneous access to several resources is required to perform some action. An example is if thread one acquires mutex one and requires mutex two to perform some action meanwhile thread two posses mutes two and requires mutex one to perform it’s own action. In This case the two threads will be deadlock in a state of perpetual slumber (unless timed).

`scoped_lock` helps by allowing us acquire several locks simultaneously:
```c++
void f()
{
	scoped_lock lck {mutex1, mutex2, mutex3};
	//..
} // implicitly releases mutex as scope exits
```
`scoped_lock` is scope limited, hence its name scoped-lock, it commands ownership of its acquired mutexes as long as the scope of its caller is still active, once that scope ends, It’s destructor releases the mutexes, for others.

`scoped_lock`, would also only proceed after all of its `mutexes` in it’s arguments have been acquired and will never block “go to sleep” holding a mutex.

locking and unlocking mutexes for acquisition and all other processes that come with it is quite low-level and sometimes very expensive. so an alternative is using a “reader-writer-lock” model where basically data is shared among numerous readers but a single writer, a construct that represents this logic is the `shared_mutex`, here the readers gain access, whereas the writer demands exclusive access.

```c++
shared_mutex mx;

void reader()
{
	shared_lock lck {mx};  // willing to share access with other readers
	//.. read
}

void writer()
{
	unique_lock lck {mx};     // needs exclusive access (write)
	//.. write
}
```

## 18.3.2 `atomics`
Honestly i don’t understand you. You will be visited later in my life.

## 18.4 Waiting for Events
Sometime a `thread`, need to wait for some kind of external event to perform some operation. The simplest “event” is time passing. The basic support for communicating using external events is provided by `conditional_variable`s, found in `<conditional_variables>`, it is a mechanism that allows one `thread` to wait for another, In particular its allows a `thread` to wait fro some condition (often called an event) to occur as the result of work done by other `threads`.

An example:
```c++
class Message {
	// implementation
};

queue<Message> mqueue;
condition_variable mcond;
mutex mmtex;

void consumer()
{
	unique_lock lck (mmtex);                       // acquire mutex
	mcond.wait(lck, []{ return !mqueue.empty(); });// release mutex and
												   // wait till queue is
												   // not empty then awake
												   // and re-acquire mutex
	auto m = mqueue.front();
	mqueue.pop();
	lck.unlock();
	// process m
}

void producer()
{
	while(true) {
		Message m;
		// fill message
		scoped_lock lck {mmtex};       // protect operations
		mqueue.push(m);
		mcond.notify_one();       // notify
	}                             // release mutex; (end of scope)
}
```
Here waiting on a `condition_variable`, releases the lock argument until the wait if over (meaning the queue is not empty) and the re-acquires it to perform its duties.

A `unique_lock` is used rather than `scoped_lock` due to 2 reasons
* we need to pas the lock to `condition_variable`, `wait()`. `scoped_lock` cannot be moved.
* We want to unlock the `mutex` protecting the condition variable before processing the message.

## 18.5 Communicating Tasks
The standard library offers facilities to operate at  a conceptual level of tasks (work to potentially be done concurrently) rather than directly at the lower level of threads and locks:
* `future` and `promise`:  return value from separate thread
* `packaged_task`: launch tasks and connect mechanisms to return result
* `async()`: launch task in a manner similar to calling a function.
These facilities are found in `<future>`

## 18.5.1 `future` and `promise`
These two enable the transfer of  value between two tasks without explicit use of a lock. The ideology is simple; if a task wants to pass a value to another, it wraps it in a `promise`, then “somehow”, the value appears in the corresponding’s thread `future`, from which it can be read.

The main purpose of a `promise` is to provide a simple `put` operation (`set_value` and `set_exception`) to match `future`’s  `get()`.

we get the value form a `future` by invoking it’s `get()`, if the value isn’t there yet is blocked until it arrives. `get()` might throw an exception if it’s value failed compute:
```c++
void f(promise<X>& px)
{
	try {
		X res;
		// ..compte val for res
		px.set_value(res);
	} catch(...) {  // oops: couldn't compute res
		px.set_exception(current_exception());
	}
}

void g(future<X>& fx)
{
	try {
		X v = fx.get();
	} catch(...) {      // someone couldn't compute v
		// handle error
	}
}
```

## 18.5.2 `packaged_task`
We can use a `packaged_task`to simplify setting up tasks connected with `futures` and promises to run on `threads`. It provides a wrapper code to put the return value or exception from the task into a `promise`. If you ask it by calling `get_future` a `packaged_task`  returns the future pertaining to that promise:
```c++
using vd_i = vector<double>::iterator;
double accum(vd_i begin, vd_i end, double init)
{
	return accumulate(begin, end, init);
}

double comp2(vector<double>& v)
{
	packaged_task pt0 {accum};
	packaged_task pt1 {accum};

	future<double> f0 {pt0.get_future()};
	future<double> f1 {pt1.get_future()};

	double *first = &v[0];
	thread t1 {move(pt0), first, first+v.size()/2, 0};
	thread t2 {move(pt1), first+v.size()/2, first+v.size(), 0};
	//..
	return f0.get() + f1.get();
}
```
`packaged_task` cannot be copied, hence the `move()`.

## 18.5.3 `async()`
To launch tasks to potentially run asynchronously, we can use `async()`, we should not even think to use `async` for tasks that share resources that need locking. 

with `async`, we don’t have to deal with locks or `mutex`, just the hope that our tasks will run concurrently. With `async`, we cannot know how many `threads` will be, that is left for `async` to know, its makes it’s decision based on the what it knows about the system resources available.

It should be noted that `async` is not just a mechanism specialized for parallel computes that yield increased performance. It can also be used to spawn tasks fro getting information from a user, leaving the “main program” active with something else:
```c++
double comp4(vector<double>& v)
{
	if( v.size < 10'000)
		return accumulate(v.begin(), v.end(), 0.0);
	
	auto first = &v[0];
	auto v_size = v.size();
	
	auto f0 = async(accum, first, first+v_size/4, 0.0);
	auto f1 = async(accum, first+v_size/4, first+v_size/2, 0.0);
	auto f2 = async(accum, first+v_size/2, frist+v_size*(3/4), 0.0);
	auto f3 = async(accum, first+v_size*(3/4), first+v_size, 0.0);
	
	return f0.get() + f1.get() + f2.get() + f3.get();
}
```

## 18.5.4 Stopping a `thread`
Sometimes, we want to stop a thread because we are no longer resulted in it’s result. Just “killing” is not ideal, because the thread can own “resources” that must be released. Instead std provides mechanisms that politely requests a thread to terminate and perform whatever clean-up it might need to do before it’s termination.

A thread can be programmed to terminate if it has `stop_token` and is requested to stop.
```c++
atomic<int> result = -1:

template <typename T>
struct Range { T* begin; T* end; };

void find(stop_token tok, const string* base, const Range<string> r,
	const string target)
{
	for(auto p = r.begin; p!= r.end && !tok.stop_requested(); ++p)
		if (match(*p, target)) {
			result = p - base;
			return;
		}
}

void user(vector<string>& vec, const string& target)
{
	auto first = &vec[0];
	auto mid = first+vec.size()/2;
	
	stop_source ss1 {};
	jthread(find, ss1.get_token(),first, Range{first, mid}, target);
	
	stop_source ss2 {};
	jthread(find, ss2.get_token(),first, Range{mid, first+vec.size()}, 
		target);
	
	while (result==-1)
		this_thread::sleep_for(10ms);
	
	// request both threads to stop
	ss1.request_stop();
	ss2.request_stop();
	
	// use result
}
```
The `stop_sources` produce `stop_token`s through which requests to stop are communicated to `threads`.

## 18.6 Coroutines
A coroutine is a function that maintains its state between calls. Its a bit like a function object, but that saving and restoring of state is implicit and complete:
```c++
generator<long long> fib()
{
	long long a = 0;
	long long b = 1;
	
	while (a<b) {
		auto next = a+b;
		co_yield next;   // save state, return valuem and wait
		a = b;
		b = next;
	}
	co_return 0; // fib to far -> next > numeric_limit<long long>::max()
			     //            .: a -> some stupidly large number
			     //            && b -> long long bound wrapped number
			     //            which will cause while(a<b) to fail
}

void user(int max)
{
	for(int i=0; i++<max;)
		cout << fib() << '';
}
```
`generator` return value is where the coroutine stores its states between calls. If we didn’t use a coroutine we would have to maintain and restore states ourselves which would have been tedious, especially for larger states and more complex computes.

A coroutine is a function that saves its stack-frame between calls. `co_yield`, returns a value and waits for the next call. `co_return` returns a value and terminates the coroutine.

They can be synchronous (caller waits) or asynchronous (callers does something else while waiting for coroutines. It is designed by and for experts with a touch of design bu committee. `generator` is not yet part of standard library, you can find good implementation from the web though like `Cppcoro`.

## 18.6.1 Cooperative Multitasking
i will reference you when needed. Good bye a tour of cpp.